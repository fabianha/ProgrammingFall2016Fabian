{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'devtools' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\Fabia\\AppData\\Local\\Temp\\RtmpQ7DRnI\\downloaded_packages\n",
      "package 'testthat' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\Fabia\\AppData\\Local\\Temp\\RtmpQ7DRnI\\downloaded_packages\n",
      "package 'roxygen2' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\Fabia\\AppData\\Local\\Temp\\RtmpQ7DRnI\\downloaded_packages\n"
     ]
    }
   ],
   "source": [
    "### Installing the Package \n",
    "\n",
    "install.packages(\"devtools\",\n",
    "                 repos='http://cran.us.r-project.org')\n",
    "install.packages(\"testthat\",\n",
    "                 repos='http://cran.us.r-project.org')\n",
    "install.packages(\"roxygen2\",\n",
    "                 repos='http://cran.us.r-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error: Directory exists and is not empty\n",
     "output_type": "error",
     "traceback": [
      "Error: Directory exists and is not empty\nTraceback:\n",
      "1. create(name)",
      "2. stop(\"Directory exists and is not empty\", call. = FALSE)"
     ]
    }
   ],
   "source": [
    "library(\"devtools\")\n",
    "# Change these to suit your needs\n",
    "mydir <- 'C:/Users/Fabia/ProgrammingFall2016Fabian/Final_Project/'\n",
    "name <- 'Simpsons'\n",
    "\n",
    "# Create the package\n",
    "setwd(mydir)\n",
    "create(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make changes to my Package\n",
    "dir.create(paste(paste(mydir, name, sep=\"Simpsons_sep\"), \"man\", sep=\"Simpsons_man\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "In scan(file = file, what = what, sep = sep, quote = quote, dec = dec, : EOF in ZeichenketteWarning message:\n",
      "In scan(file = file, what = what, sep = sep, quote = quote, dec = dec, : EOF in ZeichenketteUpdating Simpsons documentation\n",
      "Loading Simpsons\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing NAMESPACE\n",
      "Writing count_female_words.Rd\n",
      "Writing load_data.Rd\n",
      "Writing merge_and_clean.Rd\n"
     ]
    }
   ],
   "source": [
    "## This function was also created in a seperate R.file\n",
    "\n",
    "#' A Function to load the datasets. \n",
    "#'\n",
    "#' This function loads the three datasets you download from kaggle: https://www.kaggle.com/wcukierski/the-simpsons-by-the-data\n",
    "#' @export\n",
    "#' @examples\n",
    "#' load_data()\n",
    "\n",
    "load_data <- function(data_script, data_gender, data_imdb){\n",
    "    data_script <- read.csv(paste(mydir, \"simpsons_script_lines.csv\", sep=\"\"))\n",
    "    data_gender <- read.csv(paste(mydir, \"simpsons_characters.csv\", sep=\"\"))\n",
    "    data_imdb <- read.csv(paste(mydir, \"simpsons_episodes.csv\", sep=\"\"))\n",
    "}\n",
    "\n",
    "load_data(data_script, data_gender, data_imdb)\n",
    "\n",
    "setwd(paste(mydir, name, sep=\"\"))\n",
    "document()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating Simpsons documentation\n",
      "Loading Simpsons\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing NAMESPACE\n",
      "Writing count_female_words.Rd\n",
      "Writing load_data.Rd\n",
      "Writing merge_and_clean.Rd\n"
     ]
    }
   ],
   "source": [
    "#' A function to merge and clean unimportant variables and observations\n",
    "#'\n",
    "#' This function clears all unimportant oberservations and variables and merges the two important sets data_script and data_gender\n",
    "#' @export\n",
    "#' @examples\n",
    "#' merge_and_clean()\n",
    "\n",
    "merge_and_clean <- function(data_gender, data_script){\n",
    "# only observations with gender: data_gender2\n",
    "    data_gender2 <- subset(data_gender, gender == \"m\" | gender == \"f\") \n",
    "    names(data_gender2)[1] <- \"character_id\"\n",
    "# getting rid of all the trash in the big data set: data_script2\n",
    "    data_script2 <- data_script[, c(\"id\", \"number\", \"episode_id\", \"timestamp_in_ms\", \"character_id\", \"raw_character_text\", \"raw_location_text\", \"word_count\")]\n",
    "    head(data_script2)\n",
    "# merging by character_id\n",
    "    data_scgen <- merge(data_script2, data_gender2, by=\"character_id\")\n",
    "    data_scgen <- data_scgen[order(data_scgen$episode_id),]\n",
    "# cleaning the first two rows and observations without row_count \n",
    "    data_scgen <- data_scgen[3:72585, ]\n",
    "    data_scgen <- subset(data_scgen, word_count != \"\") \n",
    "    head(data_scgen)\n",
    "}\n",
    "#merge_and_clean(data_gender, data_script)\n",
    "\n",
    "setwd(paste(mydir, name, sep=\"\"))\n",
    "document()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>episode</th><th scope=col>words_male</th><th scope=col>words_female</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>1</td><td>0</td><td>16</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       "  & episode & words_male & words_female\\\\\n",
       "\\hline\n",
       "\t1 & 0 & 0 & 0\\\\\n",
       "\t2 & 1 & 0 & 16\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  episode words_male words_female\n",
       "1       0          0            0\n",
       "2       1          0           16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating Simpsons documentation\n",
      "Loading Simpsons\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing NAMESPACE\n",
      "Writing count_female_words.Rd\n",
      "Writing load_data.Rd\n",
      "Writing merge_and_clean.Rd\n"
     ]
    }
   ],
   "source": [
    "#' A Function to count the female words per episode and save it in a dataframe\n",
    "#'\n",
    "#' This function tracks the absolute number of words spoken by women. It first adds up all words and finally stores the final amount of female and male words of every episode in a new dataset. However, as the function stands now the ordering did not work very well so that it creates an incomplete summary of word counts. \n",
    "#' @export\n",
    "#' @examples\n",
    "#' Â´merge_and_clean()\n",
    "\n",
    "count_female_words <- function(i, j, data_scgen, data_wpe){\n",
    "# creating a data set to save the female \"words per episode\" (wpe)  in\n",
    "    data_scgen[,\"words_per_episode\"] <- 0\n",
    "    head(data_scgen)\n",
    "    data_wpe <- data.frame(\"episode\"=0, \"words_male\"=0, \"words_female\"=0)\n",
    "    data_wpe\n",
    "# creating setting vectors for row and episode\n",
    "    i <- 1 # episode \n",
    "    j <- 1 # row\n",
    "    addmen <- 0 \n",
    "    addwoman <- 0  \n",
    "# starting to count female words per episode\n",
    "    while(i <= 400){\n",
    "        if(data_scgen[j, 'gender'] == \"m\"){\n",
    "            addmen <- as.numeric(as.character(data_scgen[j,'word_count'])) + addmen\n",
    "            data_scgen[j,'words_per_episode'] <- addmen\n",
    "            j <- j + 1\n",
    "        }\n",
    "        if(data_scgen[j,'gender'] == \"f\"){ # If line j is a women, then add wordcount of line to sum of the words in the whole episode and add this number to column words_by_episode \n",
    "            addwoman <- as.numeric(as.character(data_scgen[j,'word_count'])) + addwoman\n",
    "            data_scgen[j,'words_per_episode'] <- addwoman\n",
    "            j <- j + 1\n",
    "        }\n",
    "        if (i != as.numeric(as.character(data_scgen[j,'episode_id']))){ # If new episode\n",
    "        infos <- c(i, addmen, addwoman) # saving the calculated word counts in the infos vector\n",
    "        data_wpe <- rbind(data_wpe, infos) # adding the infos into the dataset\n",
    "        addwoman <- 0 # setting the counting variable back to 0\n",
    "        addmen <- 0 #setting the counting variable back to 0\n",
    "        i <- as.numeric(as.character(data_scgen[j,'episode_id'])) #start over with next episode\n",
    "      }\n",
    "    }\n",
    "head(data_scgen, n=10L)\n",
    "head(data_wpe)\n",
    "}\n",
    "\n",
    "count_female_words(i,j,data_scgen, data_wpe)\n",
    "setwd(paste(mydir, name, sep=\"\"))\n",
    "document()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing Simpsons\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in if (download_method_secure()) {: Fehlender Wert, wo TRUE/FALSE nÃ¶tig ist\n",
     "output_type": "error",
     "traceback": [
      "Error in if (download_method_secure()) {: Fehlender Wert, wo TRUE/FALSE nÃ¶tig ist\nTraceback:\n",
      "1. install(name)",
      "2. install_deps(pkg, dependencies = initial_deps, upgrade = upgrade_dependencies, \n .     threads = threads, force_deps = force_deps, quiet = quiet, \n .     ...)",
      "3. dev_package_deps(pkg, repos = repos, dependencies = dependencies, \n .     type = type)",
      "4. filter_duplicate_deps(package_deps(deps, repos = repos, type = type), \n .     installing$remote_deps %||% remote_deps(pkg))",
      "5. rbind(cran_deps, remote_deps)",
      "6. package_deps(deps, repos = repos, type = type)",
      "7. cran_mirror()"
     ]
    }
   ],
   "source": [
    "setwd(mydir)\n",
    "install(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* testthat is already initialized\n"
     ]
    }
   ],
   "source": [
    "setwd(paste(mydir, name, sep=\"\"))\n",
    "devtools::use_testthat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): konnte Funktion \"context\" nicht finden\n",
     "output_type": "error",
     "traceback": "Error in eval(expr, envir, enclos): konnte Funktion \"context\" nicht finden\nTraceback:\n"
    }
   ],
   "source": [
    "# Tell R what you are testing\n",
    "context(\"count_female_words.R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>id</th><th scope=col>episode_id</th><th scope=col>original_air_date</th><th scope=col>production_code</th><th scope=col>season</th><th scope=col>number_in_season</th><th scope=col>number_in_series</th><th scope=col>us_viewers_in_millions</th><th scope=col>views</th><th scope=col>imdb_rating</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>229</th><td>33</td><td>The War of the Simpsons</td><td>1991-05-02</td><td>7F20</td><td>2</td><td>20</td><td>33</td><td>19.7</td><td>51997</td><td>7.9</td></tr>\n",
       "\t<tr><th scope=row>230</th><td>573</td><td>Bull-E</td><td>2015-05-10</td><td>TABF15</td><td>26</td><td>21</td><td>573</td><td>2.77</td><td>43978</td><td>6.8</td></tr>\n",
       "\t<tr><th scope=row>231</th><td>576</td><td>Cue Detective</td><td>2015-10-04</td><td>TABF17</td><td>27</td><td>2</td><td>576</td><td>6.02</td><td>178</td><td>6.7</td></tr>\n",
       "\t<tr><th scope=row>232</th><td>580</td><td>Friend with Benefit</td><td>2015-11-08</td><td>TABF21</td><td>27</td><td>6</td><td>580</td><td>3.48</td><td>175</td><td>6.8</td></tr>\n",
       "\t<tr><th scope=row>233</th><td>583</td><td>Barthood</td><td>2015-12-13</td><td>VABF02</td><td>27</td><td>9</td><td>583</td><td>5.97</td><td>182</td><td>8.4</td></tr>\n",
       "\t<tr><th scope=row>234</th><td>585</td><td>Teenage Mutant Milk-Caused Hurdles</td><td>2016-01-10</td><td>VABF04</td><td>27</td><td>11</td><td>585</td><td>8.33</td><td>172</td><td>6.8</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllll}\n",
       "  & id & episode_id & original_air_date & production_code & season & number_in_season & number_in_series & us_viewers_in_millions & views & imdb_rating\\\\\n",
       "\\hline\n",
       "\t229 & 33 & The War of the Simpsons & 1991-05-02 & 7F20 & 2 & 20 & 33 & 19.7 & 51997 & 7.9\\\\\n",
       "\t230 & 573 & Bull-E & 2015-05-10 & TABF15 & 26 & 21 & 573 & 2.77 & 43978 & 6.8\\\\\n",
       "\t231 & 576 & Cue Detective & 2015-10-04 & TABF17 & 27 & 2 & 576 & 6.02 & 178 & 6.7\\\\\n",
       "\t232 & 580 & Friend with Benefit & 2015-11-08 & TABF21 & 27 & 6 & 580 & 3.48 & 175 & 6.8\\\\\n",
       "\t233 & 583 & Barthood & 2015-12-13 & VABF02 & 27 & 9 & 583 & 5.97 & 182 & 8.4\\\\\n",
       "\t234 & 585 & Teenage Mutant Milk-Caused Hurdles & 2016-01-10 & VABF04 & 27 & 11 & 585 & 8.33 & 172 & 6.8\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "     id                         episode_id original_air_date production_code\n",
       "229  33            The War of the Simpsons        1991-05-02            7F20\n",
       "230 573                             Bull-E        2015-05-10          TABF15\n",
       "231 576                      Cue Detective        2015-10-04          TABF17\n",
       "232 580                Friend with Benefit        2015-11-08          TABF21\n",
       "233 583                           Barthood        2015-12-13          VABF02\n",
       "234 585 Teenage Mutant Milk-Caused Hurdles        2016-01-10          VABF04\n",
       "    season number_in_season number_in_series us_viewers_in_millions views\n",
       "229      2               20               33                   19.7 51997\n",
       "230     26               21              573                   2.77 43978\n",
       "231     27                2              576                   6.02   178\n",
       "232     27                6              580                   3.48   175\n",
       "233     27                9              583                   5.97   182\n",
       "234     27               11              585                   8.33   172\n",
       "    imdb_rating\n",
       "229         7.9\n",
       "230         6.8\n",
       "231         6.7\n",
       "232         6.8\n",
       "233         8.4\n",
       "234         6.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>episode_id</th><th scope=col>character_id</th><th scope=col>id.x</th><th scope=col>number</th><th scope=col>timestamp_in_ms</th><th scope=col>raw_character_text</th><th scope=col>raw_location_text</th><th scope=col>word_count</th><th scope=col>name</th><th scope=col>normalized_name</th><th scope=col>gender</th><th scope=col>id.y</th><th scope=col>original_air_date</th><th scope=col>production_code</th><th scope=col>season</th><th scope=col>number_in_season</th><th scope=col>number_in_series</th><th scope=col>us_viewers_in_millions</th><th scope=col>views</th><th scope=col>imdb_rating</th></tr></thead>\n",
       "<tbody>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllllllll}\n",
       "  & episode_id & character_id & id.x & number & timestamp_in_ms & raw_character_text & raw_location_text & word_count & name & normalized_name & gender & id.y & original_air_date & production_code & season & number_in_season & number_in_series & us_viewers_in_millions & views & imdb_rating\\\\\n",
       "\\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       " [1] episode_id             character_id           id.x                  \n",
       " [4] number                 timestamp_in_ms        raw_character_text    \n",
       " [7] raw_location_text      word_count             name                  \n",
       "[10] normalized_name        gender                 id.y                  \n",
       "[13] original_air_date      production_code        season                \n",
       "[16] number_in_season       number_in_series       us_viewers_in_millions\n",
       "[19] views                  imdb_rating           \n",
       "<0 rows> (or 0-length row.names)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0</li>\n",
       "\t<li>20</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0\n",
       "\\item 20\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0\n",
       "2. 20\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  0 20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#### NOT COMPLETED\n",
    "### correlation between female and imdb rating \n",
    "\n",
    "## merging data_imdb and data_scgen \n",
    "\n",
    "# dropping unimportant variables and discarting broken observations\n",
    "data_imdb <- data_imdb[1:234,1:10]\n",
    "tail(data_imdb)\n",
    "\n",
    "# renaming the id column in the imdb dataset \n",
    "names(data_imdb)[2] <- \"episode_id\"\n",
    "\n",
    "# merging the data_scgen with the data_imdb by episode id\n",
    "data_scgenim <- merge(data_scgen, data_imdb, by=\"episode_id\")\n",
    "## da ist definitv was falsch --> find solution what happened here\n",
    "head(data_scgenim)\n",
    "\n",
    "# !!! problem: we don't have data on imdb ratings for all episodes\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
